====================================================================================================
TITANS MODEL - CHECKPOINT ANALYSIS
====================================================================================================

Model File: titans_enwik8.pt
Training Batch: 2200
Validation Loss: 1.217392

====================================================================================================
MODEL CONFIGURATION
====================================================================================================
  num_tokens                    : 256
  dim                           : 384
  depth                         : 8
  segment_len                   : 32
  num_persist_mem_tokens        : 4
  num_longterm_mem_tokens       : 4
  neural_memory_layers          : (2, 4, 6)
  neural_memory_segment_len     : 4
  neural_memory_batch_size      : 128

====================================================================================================
COMPLETE MODEL WEIGHTS STRUCTURE
====================================================================================================

====================================================================================================
axial_pos_emb:
====================================================================================================
  axial_pos_emb.mlps.0.0.weight                                               [768, 1]                            torch.float32
  axial_pos_emb.mlps.0.0.bias                                                 [768]                               torch.float32
  axial_pos_emb.mlps.0.2.weight                                               [768, 768]                          torch.float32
  axial_pos_emb.mlps.0.2.bias                                                 [768]                               torch.float32
  axial_pos_emb.mlps.0.4.weight                                               [384, 768]                          torch.float32
  axial_pos_emb.mlps.0.4.bias                                                 [384]                               torch.float32
  axial_pos_emb.mlps.1.0.weight                                               [768, 1]                            torch.float32
  axial_pos_emb.mlps.1.0.bias                                                 [768]                               torch.float32
  axial_pos_emb.mlps.1.2.weight                                               [768, 768]                          torch.float32
  axial_pos_emb.mlps.1.2.bias                                                 [768]                               torch.float32
  axial_pos_emb.mlps.1.4.weight                                               [384, 768]                          torch.float32
  axial_pos_emb.mlps.1.4.bias                                                 [384]                               torch.float32

====================================================================================================
expand_streams:
====================================================================================================
  expand_streams.stream_embed                                                 [4, 384]                            torch.float32

====================================================================================================
layers.0:
====================================================================================================
  layers.0.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.0.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.0.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.0.1.static_beta                                                      [4]                                 torch.float32
  layers.0.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.0.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.0.1.norm.gamma                                                       [384]                               torch.float32
  layers.0.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.0.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.0.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.0.2.static_beta                                                      [4]                                 torch.float32
  layers.0.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.0.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.0.2.norm.gamma                                                       [384]                               torch.float32
  layers.0.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.0.5.norm.weight                                                      [384]                               torch.float32
  layers.0.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.0.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.0.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.0.6.0.weight                                                         [384]                               torch.float32
  layers.0.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.0.6.1.bias                                                           [2048]                              torch.float32
  layers.0.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.0.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.1:
====================================================================================================
  layers.1.0.static_alpha                                                     [4, 5]                              torch.float32
  layers.1.0.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.1.0.dynamic_alpha_scale                                              []                                  torch.float32
  layers.1.0.static_beta                                                      [4]                                 torch.float32
  layers.1.0.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.1.0.dynamic_beta_scale                                               []                                  torch.float32
  layers.1.0.norm.gamma                                                       [384]                               torch.float32
  layers.1.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.1.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.1.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.1.1.static_beta                                                      [4]                                 torch.float32
  layers.1.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.1.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.1.1.norm.gamma                                                       [384]                               torch.float32
  layers.1.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.1.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.1.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.1.2.static_beta                                                      [4]                                 torch.float32
  layers.1.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.1.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.1.2.norm.gamma                                                       [384]                               torch.float32
  layers.1.3.0.weight                                                         [384]                               torch.float32
  layers.1.3.1.weight                                                         [15, 384]                           torch.float32
  layers.1.3.1.bias                                                           [15]                                torch.float32
  layers.1.4.retrieve_norm.weight                                             [384]                               torch.float32
  layers.1.4.store_norm.weight                                                [384]                               torch.float32
  layers.1.4.q_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.1.4.k_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.1.4.combine_heads.weight                                             [384, 256]                          torch.float32
  layers.1.4.retrieve_gate.0.weight                                           [4, 384]                            torch.float32
  layers.1.4.memory_model.norm.gamma                                          [64]                                torch.float32
  layers.1.4.memory_model.model.weights.0                                     [64, 128]                           torch.float32
  layers.1.4.memory_model.model.weights.1                                     [128, 64]                           torch.float32
  layers.1.4.memory_model_parameters.0                                        [4, 64]                             torch.float32
  layers.1.4.memory_model_parameters.1                                        [4, 64, 128]                        torch.float32
  layers.1.4.memory_model_parameters.2                                        [4, 128, 64]                        torch.float32
  layers.1.4.to_queries.weight                                                [256, 384]                          torch.float32
  layers.1.4.to_keys.weight                                                   [256, 384]                          torch.float32
  layers.1.4.to_values.weight                                                 [256, 384]                          torch.float32
  layers.1.4.reduce_to_chunk_rep.to_attn_logits.weight                        [384, 384]                          torch.float32
  layers.1.4.reduce_to_chunk_rep.to_attn_logits.bias                          [384]                               torch.float32
  layers.1.4.to_adaptive_step.0.weight                                        [4, 384]                            torch.float32
  layers.1.4.to_adaptive_step.0.bias                                          [4]                                 torch.float32
  layers.1.4.to_momentum.0.weight                                             [4, 384]                            torch.float32
  layers.1.4.to_momentum.0.bias                                               [4]                                 torch.float32
  layers.1.4.to_layer_modulation.0.weight                                     [12, 384]                           torch.float32
  layers.1.4.to_layer_modulation.0.bias                                       [12]                                torch.float32
  layers.1.4.to_decay_factor.0.weight                                         [4, 384]                            torch.float32
  layers.1.4.to_decay_factor.0.bias                                           [4]                                 torch.float32
  layers.1.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.1.5.norm.weight                                                      [384]                               torch.float32
  layers.1.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.1.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.1.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.1.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.1.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.1.6.0.weight                                                         [384]                               torch.float32
  layers.1.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.1.6.1.bias                                                           [2048]                              torch.float32
  layers.1.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.1.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.2:
====================================================================================================
  layers.2.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.2.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.2.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.2.1.static_beta                                                      [4]                                 torch.float32
  layers.2.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.2.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.2.1.norm.gamma                                                       [384]                               torch.float32
  layers.2.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.2.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.2.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.2.2.static_beta                                                      [4]                                 torch.float32
  layers.2.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.2.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.2.2.norm.gamma                                                       [384]                               torch.float32
  layers.2.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.2.5.norm.weight                                                      [384]                               torch.float32
  layers.2.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.2.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.2.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.2.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.2.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.2.6.0.weight                                                         [384]                               torch.float32
  layers.2.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.2.6.1.bias                                                           [2048]                              torch.float32
  layers.2.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.2.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.3:
====================================================================================================
  layers.3.0.static_alpha                                                     [4, 5]                              torch.float32
  layers.3.0.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.3.0.dynamic_alpha_scale                                              []                                  torch.float32
  layers.3.0.static_beta                                                      [4]                                 torch.float32
  layers.3.0.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.3.0.dynamic_beta_scale                                               []                                  torch.float32
  layers.3.0.norm.gamma                                                       [384]                               torch.float32
  layers.3.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.3.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.3.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.3.1.static_beta                                                      [4]                                 torch.float32
  layers.3.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.3.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.3.1.norm.gamma                                                       [384]                               torch.float32
  layers.3.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.3.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.3.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.3.2.static_beta                                                      [4]                                 torch.float32
  layers.3.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.3.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.3.2.norm.gamma                                                       [384]                               torch.float32
  layers.3.3.0.weight                                                         [384]                               torch.float32
  layers.3.3.1.weight                                                         [39, 384]                           torch.float32
  layers.3.3.1.bias                                                           [39]                                torch.float32
  layers.3.4.retrieve_norm.weight                                             [384]                               torch.float32
  layers.3.4.store_norm.weight                                                [384]                               torch.float32
  layers.3.4.q_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.3.4.k_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.3.4.combine_heads.weight                                             [384, 256]                          torch.float32
  layers.3.4.retrieve_gate.0.weight                                           [4, 384]                            torch.float32
  layers.3.4.memory_model.norm.gamma                                          [64]                                torch.float32
  layers.3.4.memory_model.model.weights.0                                     [64, 128]                           torch.float32
  layers.3.4.memory_model.model.weights.1                                     [128, 64]                           torch.float32
  layers.3.4.memory_model_parameters.0                                        [4, 64]                             torch.float32
  layers.3.4.memory_model_parameters.1                                        [4, 64, 128]                        torch.float32
  layers.3.4.memory_model_parameters.2                                        [4, 128, 64]                        torch.float32
  layers.3.4.to_queries.weight                                                [256, 384]                          torch.float32
  layers.3.4.to_keys.weight                                                   [256, 384]                          torch.float32
  layers.3.4.to_values.weight                                                 [256, 384]                          torch.float32
  layers.3.4.reduce_to_chunk_rep.to_attn_logits.weight                        [384, 384]                          torch.float32
  layers.3.4.reduce_to_chunk_rep.to_attn_logits.bias                          [384]                               torch.float32
  layers.3.4.to_adaptive_step.0.weight                                        [4, 384]                            torch.float32
  layers.3.4.to_adaptive_step.0.bias                                          [4]                                 torch.float32
  layers.3.4.to_momentum.0.weight                                             [4, 384]                            torch.float32
  layers.3.4.to_momentum.0.bias                                               [4]                                 torch.float32
  layers.3.4.to_layer_modulation.0.weight                                     [12, 384]                           torch.float32
  layers.3.4.to_layer_modulation.0.bias                                       [12]                                torch.float32
  layers.3.4.to_learned_weight_residual_mix.0.weight                          [4, 384]                            torch.float32
  layers.3.4.to_learned_weight_residual_mix.0.bias                            [4]                                 torch.float32
  layers.3.4.to_decay_factor.0.weight                                         [4, 384]                            torch.float32
  layers.3.4.to_decay_factor.0.bias                                           [4]                                 torch.float32
  layers.3.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.3.5.norm.weight                                                      [384]                               torch.float32
  layers.3.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.3.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.3.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.3.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.3.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.3.6.0.weight                                                         [384]                               torch.float32
  layers.3.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.3.6.1.bias                                                           [2048]                              torch.float32
  layers.3.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.3.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.4:
====================================================================================================
  layers.4.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.4.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.4.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.4.1.static_beta                                                      [4]                                 torch.float32
  layers.4.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.4.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.4.1.norm.gamma                                                       [384]                               torch.float32
  layers.4.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.4.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.4.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.4.2.static_beta                                                      [4]                                 torch.float32
  layers.4.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.4.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.4.2.norm.gamma                                                       [384]                               torch.float32
  layers.4.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.4.5.norm.weight                                                      [384]                               torch.float32
  layers.4.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.4.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.4.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.4.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.4.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.4.6.0.weight                                                         [384]                               torch.float32
  layers.4.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.4.6.1.bias                                                           [2048]                              torch.float32
  layers.4.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.4.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.5:
====================================================================================================
  layers.5.0.static_alpha                                                     [4, 5]                              torch.float32
  layers.5.0.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.5.0.dynamic_alpha_scale                                              []                                  torch.float32
  layers.5.0.static_beta                                                      [4]                                 torch.float32
  layers.5.0.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.5.0.dynamic_beta_scale                                               []                                  torch.float32
  layers.5.0.norm.gamma                                                       [384]                               torch.float32
  layers.5.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.5.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.5.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.5.1.static_beta                                                      [4]                                 torch.float32
  layers.5.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.5.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.5.1.norm.gamma                                                       [384]                               torch.float32
  layers.5.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.5.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.5.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.5.2.static_beta                                                      [4]                                 torch.float32
  layers.5.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.5.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.5.2.norm.gamma                                                       [384]                               torch.float32
  layers.5.3.0.weight                                                         [384]                               torch.float32
  layers.5.3.1.weight                                                         [63, 384]                           torch.float32
  layers.5.3.1.bias                                                           [63]                                torch.float32
  layers.5.4.retrieve_norm.weight                                             [384]                               torch.float32
  layers.5.4.store_norm.weight                                                [384]                               torch.float32
  layers.5.4.q_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.5.4.k_norm.gamma                                                     [4, 1, 64]                          torch.float32
  layers.5.4.combine_heads.weight                                             [384, 256]                          torch.float32
  layers.5.4.retrieve_gate.0.weight                                           [4, 384]                            torch.float32
  layers.5.4.memory_model.norm.gamma                                          [64]                                torch.float32
  layers.5.4.memory_model.model.weights.0                                     [64, 128]                           torch.float32
  layers.5.4.memory_model.model.weights.1                                     [128, 64]                           torch.float32
  layers.5.4.memory_model_parameters.0                                        [4, 64]                             torch.float32
  layers.5.4.memory_model_parameters.1                                        [4, 64, 128]                        torch.float32
  layers.5.4.memory_model_parameters.2                                        [4, 128, 64]                        torch.float32
  layers.5.4.to_queries.weight                                                [256, 384]                          torch.float32
  layers.5.4.to_keys.weight                                                   [256, 384]                          torch.float32
  layers.5.4.to_values.weight                                                 [256, 384]                          torch.float32
  layers.5.4.reduce_to_chunk_rep.to_attn_logits.weight                        [384, 384]                          torch.float32
  layers.5.4.reduce_to_chunk_rep.to_attn_logits.bias                          [384]                               torch.float32
  layers.5.4.to_adaptive_step.0.weight                                        [4, 384]                            torch.float32
  layers.5.4.to_adaptive_step.0.bias                                          [4]                                 torch.float32
  layers.5.4.to_momentum.0.weight                                             [4, 384]                            torch.float32
  layers.5.4.to_momentum.0.bias                                               [4]                                 torch.float32
  layers.5.4.to_layer_modulation.0.weight                                     [12, 384]                           torch.float32
  layers.5.4.to_layer_modulation.0.bias                                       [12]                                torch.float32
  layers.5.4.to_learned_weight_residual_mix.0.weight                          [4, 384]                            torch.float32
  layers.5.4.to_learned_weight_residual_mix.0.bias                            [4]                                 torch.float32
  layers.5.4.to_decay_factor.0.weight                                         [4, 384]                            torch.float32
  layers.5.4.to_decay_factor.0.bias                                           [4]                                 torch.float32
  layers.5.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.5.5.norm.weight                                                      [384]                               torch.float32
  layers.5.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.5.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.5.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.5.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.5.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.5.6.0.weight                                                         [384]                               torch.float32
  layers.5.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.5.6.1.bias                                                           [2048]                              torch.float32
  layers.5.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.5.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.6:
====================================================================================================
  layers.6.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.6.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.6.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.6.1.static_beta                                                      [4]                                 torch.float32
  layers.6.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.6.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.6.1.norm.gamma                                                       [384]                               torch.float32
  layers.6.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.6.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.6.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.6.2.static_beta                                                      [4]                                 torch.float32
  layers.6.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.6.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.6.2.norm.gamma                                                       [384]                               torch.float32
  layers.6.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.6.5.norm.weight                                                      [384]                               torch.float32
  layers.6.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.6.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.6.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.6.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.6.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.6.6.0.weight                                                         [384]                               torch.float32
  layers.6.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.6.6.1.bias                                                           [2048]                              torch.float32
  layers.6.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.6.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
layers.7:
====================================================================================================
  layers.7.1.static_alpha                                                     [4, 5]                              torch.float32
  layers.7.1.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.7.1.dynamic_alpha_scale                                              []                                  torch.float32
  layers.7.1.static_beta                                                      [4]                                 torch.float32
  layers.7.1.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.7.1.dynamic_beta_scale                                               []                                  torch.float32
  layers.7.1.norm.gamma                                                       [384]                               torch.float32
  layers.7.2.static_alpha                                                     [4, 5]                              torch.float32
  layers.7.2.dynamic_alpha_fn                                                 [384, 5]                            torch.float32
  layers.7.2.dynamic_alpha_scale                                              []                                  torch.float32
  layers.7.2.static_beta                                                      [4]                                 torch.float32
  layers.7.2.dynamic_beta_fn                                                  [384]                               torch.float32
  layers.7.2.dynamic_beta_scale                                               []                                  torch.float32
  layers.7.2.norm.gamma                                                       [384]                               torch.float32
  layers.7.5.persistent_memory                                                [2, 8, 4, 64]                       torch.float32
  layers.7.5.norm.weight                                                      [384]                               torch.float32
  layers.7.5.rotary_emb.freqs                                                 [32]                                torch.float32
  layers.7.5.to_qkv.weight                                                    [1536, 384]                         torch.float32
  layers.7.5.to_out.weight                                                    [384, 512]                          torch.float32
  layers.7.5.to_learned_v_mix.0.weight                                        [8, 384]                            torch.float32
  layers.7.5.to_learned_v_mix.0.bias                                          [8]                                 torch.float32
  layers.7.6.0.weight                                                         [384]                               torch.float32
  layers.7.6.1.weight                                                         [2048, 384]                         torch.float32
  layers.7.6.1.bias                                                           [2048]                              torch.float32
  layers.7.6.3.weight                                                         [384, 1024]                         torch.float32
  layers.7.6.3.bias                                                           [384]                               torch.float32

====================================================================================================
longterm_mems:
====================================================================================================
  longterm_mems                                                               [4, 384]                            torch.float32

====================================================================================================
norm:
====================================================================================================
  norm.weight                                                                 [384]                               torch.float32

====================================================================================================
to_logits:
====================================================================================================
  to_logits.weight                                                            [256, 384]                          torch.float32

====================================================================================================
token_emb:
====================================================================================================
  token_emb.weight                                                            [256, 384]                          torch.float32

====================================================================================================
MODEL STATISTICS
====================================================================================================

Total Parameters: 19,790,635
Model Size: 75.50 MB
Number of Layers: 8 (depth from config)
Hidden Dimension: 384
Number of Streams: 4 (inferred from shapes)
Attention Heads: 8 (inferred from shapes)

====================================================================================================
LAYER BREAKDOWN
====================================================================================================
  Layer 0: 1,978,836 parameters
  Layer 1: 2,626,141 parameters
  Layer 2: 1,981,916 parameters
  Layer 3: 2,636,921 parameters
  Layer 4: 1,981,916 parameters
  Layer 5: 2,646,161 parameters
  Layer 6: 1,981,916 parameters
  Layer 7: 1,981,916 parameters

====================================================================================================
COMPONENT BREAKDOWN
====================================================================================================
  Feed-Forward Networks         :    9,468,636 parameters (47.84%)
  Attention Weights             :    7,176,192 parameters (36.26%)
  Positional Embedding          :    1,774,848 parameters ( 8.97%)
  Other                         :    1,036,885 parameters ( 5.24%)
  Neural Memory                 :      246,720 parameters ( 1.25%)
  TITAN Alpha/Beta              :       39,610 parameters ( 0.20%)
  Persistent Memory             :       32,768 parameters ( 0.17%)
  Normalization Layers          :       13,440 parameters ( 0.07%)
  Stream Embeddings             :        1,536 parameters ( 0.01%)

====================================================================================================
